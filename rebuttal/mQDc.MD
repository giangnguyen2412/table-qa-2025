> 1. Given POS’s sequential nature and reliance on intermediate tables as inputs for each step, what are the efficiency implications?

Thank you for this valuable question!

To analyze the efficiency of POS, we followed the analysis used in the 
Chain-of-Table paper (Sec 4.5), using the total number of LLM queries as the measure of efficiency.
When comparing POS to other baselines, including Binder (Cheng et al., 2022), 
Dater (Ye et al., 2023), and CoTable (Wang et al., 2024), we observe a distinct advantage: 
POS does not rely on self-consistency (generating multiple candidate answers and selecting the most frequent or consistent one to improve accuracy) 
during either the planning or SQL generation steps, 
whereas all three baseline methods use self-consistency to boost accuracy. 
This helps POS significantly reduce the number of LLM queries needed per sample (to only `4`), 
making POS the most efficient among these baselines. See the Table below.

Table: Efficiency analysis on WikiTQ

| Method                      | Self-consistency | LLM queries | Breakdown LLM queries                                                | Database queries |
|-----------------------------|------------------|-------------|----------------------------------------------------------------------|------------------|
| Binder (Cheng et al., 2022) | Yes              | 50          | Generate Neural-SQL: 50                                              | 50               |
| Dater (Ye et al., 2023)     | Yes              | 100         | Decompose Table: 40; Generate Cloze: 20; Generate SQL: 20; Query: 20 | 20               |
| CoTable (Wang et al., 2024) | Yes              | ≤25         | DynamicPlan: ≤5; GenerateArgs: ≤19; Query: 1                         | 5                |
| Plan-of-SQLs (Ours)         | No               | 4           | Planning: 2 Generate SQL: 2                                          | 2                |

Additionally, we propose to compare the efficiency of these methods based on the number of 
table transformations, as a higher count leads to increased workload on the local database 
when handling Table QA tasks (i.e., higher number of queries to the table database). 
In this regard, POS also demonstrates a clear advantage over the baselines.
In particular, while methods like Chain-of-Table rely on a predefined set of `5` 
operations (e.g., add column, select column/row, group column, sort column) to maintain 
consistency, POS requires only `2` database queries on average, making it the most efficient 
approach in terms of transformation operations. Similarly, Binder and Dater require 
significantly greater numbers of operations, with Binder doing table transforms `50` times 
and Dater doing slot-filling `20` times using SQLs.


> 2. In Appendix C, the ablation study discusses changes in interpretability after removing different modules, but no quantitative metrics are provided to measure these changes. Could the authors include specific interpretability metrics to quantify the impact of each module?

Thank you for highlighting this point! 
We would like to justify that the main goal of this ablation study is to study how different components affect POS QA accuracy, not interpretability. 
However, to address interpretability quantitatively, we can use the fallback rate as a proxy 
metric. In our approach, fallback occurs when the model defaults to an end-to-end QA approach, 
which lacks interpretability (as shown in Fig. 1). Thus, a lower fallback rate indicates that 
POS can solve more queries using SQL, providing a fully interpretable decision-making process.

As shown in Table 4, modules like NL Planning and Text-to-SQL significantly contribute to 
interpretability, with their removal increasing the fallback rate to 40-50%. 
Removing NL Planning results in SQL-only steps (in lieu of natural-language steps Fig. 7) will 
diminish interpretability, as shown by lower performance of Text-to-SQL compared to POS in the Forward Simulation experiment (Table 1). 

Similarly, without Text-to-SQL, LLM performs table transformations directly in natural language without using SQLs, which is less interpretable due to the inherent LLM black-box reasoning.

When the Atomicity is ablated out, the fallback rate does not change much. To justify the change in interpretability, we visualized the plans generated by NL Planner in Fig.8 and Fig.9 and conducted a qualitative analysis (on 200 samples), which hinted that explanations were indeed less interpretable in this setup due to complex steps (see Fig.8 and 9’s captions). 

> 3. There appear to be issues with Figure 6. The function parameters in Step 1 are incorrect, and there is an unintended split in the image for Step 3.

Thank you for this very detailed comment! In Fig.6, we actually visualize exactly the row indices given by the Chain-of-Table method (i.e. row 1, row 2, row 3, row 4, row 8). In Chain-of-Table, they ignore the header row (column names), and start with index 0 (here refers to game 1). Please note that in human studies, we explicitly told users that CoTable row indices start from 0 and we do not count the header row.
Yet, we agree with the Reviewer that we can adjust these indices to make the visualization less confusing and aligned with other methods (e.g. DATER in Fig.5). Regarding the unintended split, this was an error in converting the visualization to PDF for insertion into Overleaf. We will address both issues in the final version. Again, thanks for the great comment!
Please let us know if we can make any further efforts to address your concerns!
